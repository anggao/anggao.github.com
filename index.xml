<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Ang's Blog</title><link>http://anggao.github.io/</link><description>Ang's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 01 Dec 2019 21:57:40 +0800</lastBuildDate><atom:link href="http://anggao.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Basic Markdown Syntax</title><link>http://anggao.github.io/hugo/</link><pubDate>Sun, 01 Dec 2019 21:57:40 +0800</pubDate><author>Author</author><guid>http://anggao.github.io/hugo/</guid><description>&lt;p>This article offers a sample of basic Markdown syntax that can be used in Hugo content files.&lt;/p></description></item><item><title>Kind Cluster issue</title><link>http://anggao.github.io/2020-03-01-kind-cluster-xfs/</link><pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2020-03-01-kind-cluster-xfs/</guid><description>https://docs.docker.com/storage/storagedriver/overlayfs-driver/
The overlay and overlay2 drivers are supported on xfs backing filesystems, but only with d_type=true enabled.
xfs_info /
ascii-ci=0 ftype=0
The 3rd column of the 6th line of the xfs_info output is the most interesting because it contains the parameter ftype which should be 1. When ftype is 0, d_type support is disabled. When it is 1, d_type support is enabled and you&amp;rsquo;re safe to use the overlay(2) storage driver with Docker on an XFS filesystem.</description></item><item><title>Kubernetes Note 05</title><link>http://anggao.github.io/2019-02-20-k8s-05/</link><pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-02-20-k8s-05/</guid><description>对于一个 Deployment 所管理的 Pod，它的 ownerReference 是 ReplicaSet ReplicaSet 负责通过控制器模式，保证系统中 Pod 的个数永远等于指定的个数 Deployment 只允许容器的 restartPolicy=Always 水平扩展 $ kubectl scale deployment nginx-deployment --replicas=4 滚动更新 R</description></item><item><title>Kubernetes Note 04</title><link>http://anggao.github.io/2019-02-18-k8s-04/</link><pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-02-18-k8s-04/</guid><description>容器健康检查和恢复机制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion:v1kind:Podmetadata:labels:test:livenessname:test-liveness-execspec:containers:- name:livenessimage:busyboxargs:- /bin/sh- -c- touch/tmp/healthy;sleep30;rm-rf/tmp/healthy;sleep600livenessProbe:exec:command:- cat- /tmp/healthyinitialDelaySeconds:5periodSeconds:5 Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去</description></item><item><title>Kubernetes Note 03</title><link>http://anggao.github.io/2019-02-16-k8s-03/</link><pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-02-16-k8s-03/</guid><description>使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern） 1 2</description></item><item><title>Kubernetes Note 02</title><link>http://anggao.github.io/2019-02-11-k8s-02/</link><pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-02-11-k8s-02/</guid><description>Kubernetes 项目的架构，跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成 Master 节点 负责 API 服务的 kube-apiserver 负责调度的 kube-scheduler 负责容器编排的 kube-controller-manager 集群的持久化数据，则由 kube-apiserver 处理后</description></item><item><title>Kubernetes Note 01</title><link>http://anggao.github.io/2019-02-10-k8s-01/</link><pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-02-10-k8s-01/</guid><description>容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界” Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图</description></item><item><title>Spark Note 01</title><link>http://anggao.github.io/2019-01-23-spark-note-01/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-01-23-spark-note-01/</guid><description>Driver A Spark driver (aka an application’s driver process) is a JVM process that hosts SparkContext for a Spark application. It is the master node in a Spark application. It splits a Spark application into tasks and schedules them to run on executors. A driver is where the task scheduler lives and spawns tasks across workers. A driver coordinates workers and overall execution of tasks. It hosts Web UI for the environment.</description></item><item><title>Scala Note 01</title><link>http://anggao.github.io/2019-01-22-scala-note-01/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2019-01-22-scala-note-01/</guid><description>当对一个数取模时，可以等价 a%b=a-a/b*b 1 2 println(-10%3)//-10%3=(-10)-(-3)*3=-10+9=-1 println(-10%-3)//-10%-3=(-10)-(3)*-3=-10+9=-1 在 scala 中支持代码块，返回值 1 2 3 val res = { if (num &amp;gt; 1) &amp;#34;ok&amp;#34; else 100 } Scala 不支持三目运算符 , 在 Scala 中使用 if – else 的方式实现 1 val</description></item><item><title>Keeping a fork up to date</title><link>http://anggao.github.io/2018-03-03-git-fork-sync/</link><pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2018-03-03-git-fork-sync/</guid><description>Clone your fork: 1 git clone git@github.com:YOUR-USERNAME/YOUR-FORKED-REPO.git Add remote from original repository in your forked repository: 1 2 3 cd into/cloned/fork-repo git remote add upstream git://github.com/ORIGINAL-DEV-USERNAME/REPO-YOU-FORKED-FROM.git git fetch upstream Updating your fork from original repo to keep up with their changes: 1 git pull upstream master</description></item></channel></rss>