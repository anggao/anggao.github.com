<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>hadoop - Tag - Ang's Blog</title><link>http://anggao.github.io/tags/hadoop/</link><description>hadoop - Tag - Ang's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 17 Oct 2015 00:00:00 +0000</lastBuildDate><atom:link href="http://anggao.github.io/tags/hadoop/" rel="self" type="application/rss+xml"/><item><title>Hadoop: Build Apache Hadoop 2.5.2 in CentOS 6.7</title><link>http://anggao.github.io/2015-10-17-hadoop-build-apache-hadoop-252-in-centos-67/</link><pubDate>Sat, 17 Oct 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-10-17-hadoop-build-apache-hadoop-252-in-centos-67/</guid><description>Install build tools 1 2 3 4 5 6 yum install gcc yum install gcc-c++ yum install make yum install cmake yum install openssl-devel yum install ncurses-devel Install Maven 1 2 3 4 5 wget http://ftp.heanet.ie/mirrors/www.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz tar -xzvf apache-maven-3.3.3-bin.tar.gz export MAVEN_HOME=/opt/apache-maven-3.3.3 export PATH=$MAVEN_HOME/bin:$PATH mvn -v Install protobuf 1 2 3 4 5 6 7 8 wget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.bz2 tar -xjvf protobuf-2.</description></item><item><title>Hadoop 1.x Cluster Setup (MapReduce only)</title><link>http://anggao.github.io/2015-10-05-hadoop-1x-cluster-setup-mapreduce-only/</link><pubDate>Mon, 05 Oct 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-10-05-hadoop-1x-cluster-setup-mapreduce-only/</guid><description><![CDATA[Purpose This document describes how to set up and configure a 3-nodes Hadoop 1.x MapReduce cluster.
For HDFS setup Hadoop 1.x Cluster Setup (HDFS only)
Hadoop MapReduce Config  config mapred-site.xml  1 2 3 4 5 6  &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;node1:9001&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;    copy configs to slaves  1 2 3  # From node1 scp /home/hadoop-1.2/conf/* root@node2:/home/hadoop-1.2/conf scp /home/hadoop-1.2/conf/* root@node3:/home/hadoop-1.2/conf    start MapReduce  1  /home/hadoop-1.]]></description></item><item><title>Hadoop 1.x Cluster Setup (HDFS only)</title><link>http://anggao.github.io/2015-09-20-hadoop-1x-cluster-setup-hdfs-only/</link><pubDate>Sun, 20 Sep 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-09-20-hadoop-1x-cluster-setup-hdfs-only/</guid><description>Purpose This document describes how to set up and configure a 3-nodes Hadoop 1.x Distributed File System (HDFS) cluster.
Prerequisites 3 CentOS 6.x VMs In the following examples I have three nodes: node1: 192.168.0.11 node2: 192.168.0.12 node3: 192.168.0.13 The finally setup will be: NameNode: node1 Secondary NN: node2 DataNode: node2 node3 Steps Followings steps are for: CentOS 6.7, Hadoop-1.2.1
Installing Software (3 nodes) Install Java 7 1 2 3 4 5 tar -xzvf jdk-7u79-linux-x64.</description></item><item><title>Install Sqoop on Ubuntu 14.04 and Hadoop 2.6.0</title><link>http://anggao.github.io/2015-06-27-install-sqoop-on-ubuntu-1404-and-hadoop-260/</link><pubDate>Sat, 27 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-27-install-sqoop-on-ubuntu-1404-and-hadoop-260/</guid><description>Followings steps are for: Ubuntu 14.04 LTS, Hadoop-2.6.0
For Hadoop 2.6.0 Installation on Ubuntu 14.04
Scoop Installation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 wget http://apache.proserve.nl/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz tar -xzvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6 sudo vim /etc/profile SQOOP_HOME=/home/anggao/sqoop-1.4.6 export SQOOP_HOME export PATH=$PATH:$SQOOP_HOME/bin source /etc/profile # obtain JDBC driver cd $SQOOP_HOME/lib wget https://jdbc.postgresql.org/download/postgresql-9.4-1201.jdbc4.jar sqoop version Scoop examples 1 2 3 4 5 6 7 8 9 # transfer data from PostgreSQL to HDFS sqoop import --connect jdbc:postgresql://localhost:5432/dev-db --table tableName --username userName --password randomPassword hadoop fs -cat department/part-m-00000 # transfer data from PostgreSQL to Hive sqoop import --connect jdbc:postgresql://localhost:5432/dev-db --username userName --password randomPassword --table tableName --hive-import -m 1 # transfer data from Hive to PostgreSQL sqoop export --connect jdbc:postgresql://localhost:5432/dev-db --username userName --password randomPassword --table postgreTableName --export-dir /user/hive/warehouse/hiveTableName --input-fields-terminated-by &amp;#39;\0x001&amp;#39;</description></item><item><title>Install Hive on Ubuntu 14.04 and Hadoop 2.6.0</title><link>http://anggao.github.io/2015-06-26-install-hive-on-ubuntu-1404-and-hadoop-260/</link><pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-26-install-hive-on-ubuntu-1404-and-hadoop-260/</guid><description>Followings steps are for: Ubuntu 14.04 LTS, Hadoop-2.6.0
For Hadoop 2.6.0 Installation on Ubuntu 14.04
Download and unpack Hive 1 2 wget http://mirror.tcpdiag.net/apache/hive/stable/apache-hive-1.2.0-bin.tar.gz tar -xzvf apache-hive-1.2.0-bin.tar.gz Configure environment variables 1 2 3 4 5 6 7 sudo vim /etc/profile HIVE_HOME=/home/anggao/hive-1.2.0 export HIVE_HOME export PATH=$PATH:$HIVE_HOME/bin source /etc/profile Create folders in Hadoop file system 1 2 3 4 hadoop fs -mkdir /tmp hadoop fs -mkdir /user/hive/warehouse hadoop fs -chmod g+w /tmp hadoop fs -chmod g+w /user/hive/warehouse Hive CLI 1 2 3 4 5 # Notice there is an issue with Hive 1.</description></item><item><title>Run Examples in Oozie</title><link>http://anggao.github.io/2015-06-24-run-examples-in-oozie/</link><pubDate>Wed, 24 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-24-run-examples-in-oozie/</guid><description> Extract Examples Tarball file 1 2 cd $OOZIE_HOME tar -xzvf oozie-examples.tar.gz Edit job.properties file 1 2 3 4 5 6 7 cd examples/apps/map-reduce vim job.properties nameNode=hdfs://YARN001:8020 jobTracker=YARN001:8032 queueName=default examplesRoot=examples Copy examples into HDFS 1 2 3 hadoop fs -put examples examples cd $OOZIE_HOME oozie job -oozie http://localhost:11000/oozie -config $OOZIE_HOME/examples/apps/map-reduce/job.properties -run</description></item><item><title>Apache Oozie Installation on Ubuntu</title><link>http://anggao.github.io/2015-06-23-apache-oozie-installation-on-ubuntu/</link><pubDate>Tue, 23 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-23-apache-oozie-installation-on-ubuntu/</guid><description>Followings steps are for: Ubuntu 14.04 LTS, Hadoop-2.6.0
Prerequisites: Ubuntu 14.04 Java JDK 1.7 Maven 3.0.5 Hadoop 2.6.0 Install Oozie 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # install Maven sudo apt-get update sudo apt-get install maven # build Oozie wget http://mirrors.whoishostingthis.com/apache/oozie/4.1.0/oozie-4.1.0.tar.gz tar -xzvf oozie-4.</description></item><item><title>Hadoop: HDFS Shell Commands</title><link>http://anggao.github.io/2015-06-22-hadoop-hdfs-shell-commands/</link><pubDate>Mon, 22 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-22-hadoop-hdfs-shell-commands/</guid><description><![CDATA[1 2 3 4 5 6  bin/hdfs dfs run a filesystem command on the file systems supported in Hadoop. dfsadmin run a DFS admin client fsck run a DFS filesystem checking utility balancer run a cluster balancing utility   1 2 3 4 5 6 7  bin/hdfs dfsadmin [-safemode &lt;enter | leave | get | wait&gt;] [-saveNamespace] [-refreshNodes] [-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;] [-setSpaceQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  anggao@ubuntu:~/hadoop-2.]]></description></item><item><title>Hadoop: Setting up a Single Node Cluster</title><link>http://anggao.github.io/2015-06-21-hadoop-setting-up-a-single-node-cluster/</link><pubDate>Sun, 21 Jun 2015 00:00:00 +0000</pubDate><author>Author</author><guid>http://anggao.github.io/2015-06-21-hadoop-setting-up-a-single-node-cluster/</guid><description>Purpose This document describes how to set up and configure a single-node Hadoop installation so that you can quickly perform simple operations using Hadoop MapReduce and the Hadoop Distributed File System (HDFS).
Prerequisites GNU/Linux Java must be installed. Recommended Java versions are described at HadoopJavaVersions ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons. A Hadoop distribution, download a recent stable release from one of the Apache Download Mirrors Installing Software Followings steps are for: Ubuntu 14.</description></item></channel></rss>